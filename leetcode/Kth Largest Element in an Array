using min heap
class Solution {
    public int findKthLargest(int[] nums, int k) {
        PriorityQueue<Integer> heap = new PriorityQueue<>();
        for (int num: nums) {
            heap.add(num);
            if (heap.size() > k) {
                heap.remove();
            }
        }
        
        return heap.peek();
    }
}
Given n as the length of nums,

Time complexity: O(n⋅logk)

Operations on a heap cost logarithmic time relative to its size. Because our heap is limited to a size of k, operations cost at most O(logk). We iterate over nums, performing one or two heap operations at each iteration.

We iterate n times, performing up to logk work at each iteration, giving us a time complexity of O(n⋅logk).

Because k≤n, this is an improvement on the previous approach.

Space complexity: O(k)
The heap uses O(k) space.

optimised using quick select
class Solution {
    public int findKthLargest(int[] nums, int k) {
        List<Integer> list = new ArrayList<>();
        for (int num: nums) {
            list.add(num);
        }
        
        return quickSelect(list, k);
    }
    
    public int quickSelect(List<Integer> nums, int k) {
        int pivotIndex = new Random().nextInt(nums.size());
        int pivot = nums.get(pivotIndex);
        
        List<Integer> left = new ArrayList<>();
        List<Integer> mid = new ArrayList<>();
        List<Integer> right = new ArrayList<>();
        
        for (int num: nums) {
            if (num > pivot) {
                left.add(num);
            } else if (num < pivot) {
                right.add(num);
            } else {
                mid.add(num);
            }
        }
        
        if (k <= left.size()) {
            return quickSelect(left, k);
        }
        
        if (left.size() + mid.size() < k) {
            return quickSelect(right, k - left.size() - mid.size());
        }
        
        return pivot;
    }
}
Given n as the length of nums,

Time complexity: O(n) on average, O(n^2) in the worst case

Each call we make to quickSelect will cost O(n) since we need to iterate over nums to create left, mid, and right.
The number of times we call quickSelect is dependent on how the pivots are chosen. The worst pivots to choose are the 
extreme (greatest/smallest) ones because they reduce our search space by the least amount. Because we are randomly 
generating pivots, we may end up calling quickSelect O(n) times, leading to a time complexity of O(n^2).
However, the algorithm mathematically almost surely has a linear runtime. For any decent size of nums,
the probability of the pivots being chosen in a way that we need to call quickSelect O(n) times is so low that we can ignore it.

On average, the size of nums will decrease by a factor of ~2 on each call. You may think: that means we call quickSelect O(logn) times, wouldn't that give us a time complexity of O(n⋅logn)? Well, each successive call to quickSelect would also be on a nums that is a factor of ~2 smaller. This recurrence can be analyzed using the master theorem with a = 1, b = 2, k = 1:

T(n)=T( 2n)+O(n)=O(n)

Space complexity: O(n)

We need O(n) space to create left, mid, and right. Other implementations of Quickselect can avoid creating these three in memory, but in the worst-case scenario, those implementations would still require O(n) space for the recursion call stack.


